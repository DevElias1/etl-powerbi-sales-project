# C√≥digo completo para leitura de arquivos + corre√ß√£o da coluna Data

import os
import pandas as pd
import xml.etree.ElementTree as ET

# Pasta onde o usu√°rio coloca os arquivos
PASTA = "dados_entrada/"

# Criar a pasta automaticamente caso ela n√£o exista
if not os.path.exists(PASTA):
    os.makedirs(PASTA)
    print(f"üìÅ Pasta criada automaticamente: {PASTA}")
    print("‚ö† Coloque seus arquivos dentro dela e execute novamente o script.\n")

 
# FUN√á√ÉO: Ler arquivo por extens√£o

def ler_arquivo(caminho):
    extensao = caminho.split(".")[-1].lower()

    try:
        if extensao in ["xlsx", "xls"]:
            return pd.read_excel(caminho)

        elif extensao == "csv":
            return pd.read_csv(caminho)

        elif extensao == "json":
            return pd.read_json(caminho)

        elif extensao == "txt":
            # tenta separador ; ou ,
            try:
                return pd.read_csv(caminho, sep=";")
            except:
                return pd.read_csv(caminho, sep=",")

        elif extensao == "xml":
            tree = ET.parse(caminho)
            root = tree.getroot()

            data = []
            for child in root:
                linha = {}
                for elem in child:
                    linha[elem.tag] = elem.text
                data.append(linha)

            return pd.DataFrame(data)

        else:
            print(f"‚ö† Formato n√£o suportado: {caminho}")
            return None

    except Exception as e:
        print(f"‚ùå Erro ao abrir {caminho}: {e}")
        return None


# FUN√á√ÉO: Carregar todos os arquivos da pasta

def carregar_arquivos():
    arquivos = os.listdir(PASTA)
    tabelas = {}

    if not arquivos:
        print("‚ö† Nenhum arquivo encontrado na pasta dados_entrada/")
        print("‚û° Coloque seus arquivos e execute novamente.")
        return {}

    print("üîç Procurando arquivos na pasta dados_entrada...\n")

    for arquivo in arquivos:
        caminho = os.path.join(PASTA, arquivo)

        if not os.path.isfile(caminho):
            continue

        print(f"‚û° Lendo arquivo: {arquivo}")
        df = ler_arquivo(caminho)

        if df is not None:
            nome = arquivo.split(".")[0]  # nome da tabela
            tabelas[nome] = df
            print(f"   ‚úÖ Arquivo carregado: {arquivo} ‚Üí DataFrame '{nome}'\n")

    return tabelas


# EXECU√á√ÉO PRINCIPAL

tabelas = carregar_arquivos()

if tabelas:
    print("\nüìå DATAFRAMES CARREGADOS:")
    for nome, df in tabelas.items():
        print(f"\n--- {nome.upper()} ---")
        print(df.head())
        print("\nInforma√ß√µes:")
        print(df.info())
        print("\nValores faltantes:")
        print(df.isnull().sum())


# ------------------------------------------------------------
# EXPLICA√á√ÉO DETALHADA (COMENT√ÅRIOS) ‚Äî Cole isto abaixo do seu c√≥digo
# ------------------------------------------------------------
#
# Resumo r√°pido:
# Este arquivo tem 3 responsabilidades principais:
#  1) Criar/verificar a pasta de entrada (dados_entrada/)
#  2) Varredura dessa pasta e leitura autom√°tica de arquivos por extens√£o
#  3) Expor os DataFrames carregados para uso posterior no ETL
#
# Abaixo, linha a linha / bloco a bloco: o que cada biblioteca e trecho faz,
# por que est√° ali, alternativas e boas pr√°ticas.
#
# ------------------------------------------------------------
# BIBLIOTECAS USADAS
# ------------------------------------------------------------
# import os
#   - Biblioteca padr√£o do Python para interagir com o sistema operacional.
#   - Usamos para listar arquivos (os.listdir), juntar caminhos (os.path.join),
#     criar pastas (os.makedirs) e verificar "isfile" e exist√™ncia de diret√≥rios.
#   - Alternativas/observa√ß√µes: manter, pois √© a forma correta e port√°til.
#
# import pandas as pd
#   - Pandas √© a biblioteca principal de manipula√ß√£o de dados em Python.
#   - Fornece DataFrame: estrutura tabular (linhas x colunas) muito usada para ETL.
#   - Fun√ß√µes importantes usadas: read_excel, read_csv, read_json, DataFrame(...)
#   - Observa√ß√µes:
#       * Para Excel, pandas usa engines (ex.: "openpyxl") por tr√°s ‚Äî se tiver erro, instale
#         openpyxl (pip install openpyxl).
#       * Para arquivos muito grandes (> mem√≥ria), considerar Dask, Vaex, ou Spark.
#
# import xml.etree.ElementTree as ET
#   - Biblioteca padr√£o para parse de XML.
#   - Aqui usamos para transformar um XML simples em uma lista de dicion√°rios e criar DataFrame.
#   - Observa√ß√µes:
#       * Para XMLs grandes/complexos, usar lxml (mais r√°pido e robusto).
#       * A estrutura do XML precisa ser regular para virar tabela facilmente.
#
# ------------------------------------------------------------
# VARI√ÅVEIS E CONFIGURA√á√ÉO
# ------------------------------------------------------------
# PASTA = "dados_entrada/"
#   - Caminho relativo onde o usu√°rio joga os arquivos.
#   - Pode ser absoluto se preferir (ex.: r"C:\meu_projeto\dados_entrada").
#   - Boa pr√°tica: configurar via vari√°vel de ambiente ou CLI/argparse para flexibilidade.
#
# Cria√ß√£o autom√°tica da pasta:
#   - if not os.path.exists(PASTA): os.makedirs(PASTA)
#   - Evita FileNotFoundError e melhora UX: o script cria a pasta e pede para o usu√°rio colocar arquivos.
#
# ------------------------------------------------------------
# FUN√á√ÉO ler_arquivo(caminho)
# ------------------------------------------------------------
# Objetivo:
#   - Receber um caminho de arquivo e retornar um pandas.DataFrame com os dados.
#
# Como ela identifica o formato:
#   - extensao = caminho.split(".")[-1].lower()
#   - Simples e eficaz, mas aten√ß√£o: arquivos sem extens√£o ou com m√∫ltiplos pontos podem confundir.
#
# Suporte no c√≥digo:
#   - Excel: extensao in ["xlsx", "xls"] -> pd.read_excel(caminho)
#       * pd.read_excel lida com m√∫ltiplas sheets (por padr√£o l√™ a primeira); para ler sheet espec√≠fica:
#         pd.read_excel(caminho, sheet_name="NomeDaPlanilha")
#       * Se precisar performace, considerar engine openpyxl (para xlsx) ou xlrd (antigo).
#
#   - CSV: pd.read_csv(caminho)
#       * Suporta par√¢metros: sep, encoding, parse_dates, dtype, etc.
#       * Aten√ß√£o com encoding (utf-8 vs latin-1). Se der erro, tente encoding="latin-1".
#
#   - JSON: pd.read_json(caminho)
#       * JSON tabular direto funciona; JSON aninhado pode precisar de json_normalize.
#
#   - TXT: tentamos dois separadores (;) e depois (,)
#       * Boa heur√≠stica para arquivos .txt que na pr√°tica s√£o CSVs com separadores variados.
#       * Pode-se estender para detectar separador automaticamente (ex.: csv.Sniffer).
#
#   - XML: usamos xml.etree.ElementTree para parsear e extrair elementos em lista de dicion√°rios
#       * Depois transformamos em pd.DataFrame(data)
#       * Limita√ß√£o: funciona melhor com XML estruturado como uma lista de registros.
#
# Tratamento de erros:
#   - try/except envolvendo cada leitura para capturar problemas e n√£o quebrar todo o loop.
#   - Retorna None para formatos n√£o suportados ou em caso de erro, permitindo o script continuar.
#
# Avisos:
#   - Arquivos bin√°rios ou formatos complexos (parquet, avro, parquet via pyarrow) n√£o est√£o no script.
#   - Para parquet, usar pd.read_parquet(caminho) e instalar pyarrow/fastparquet.
#
# ------------------------------------------------------------
# FUN√á√ÉO carregar_arquivos()
# ------------------------------------------------------------
# Objetivo:
#   - Listar todos os arquivos em PASTA, chamar ler_arquivo em cada um e montar um dicion√°rio
#     {nome_base_do_arquivo: DataFrame}.
#
# L√≥gica:
#   - arquivos = os.listdir(PASTA)
#   - testar se a lista est√° vazia e avisar o usu√°rio
#   - iterar, montar caminho absoluto com os.path.join(PASTA, arquivo)
#   - pular entradas que n√£o s√£o arquivos (pastas)
#   - para cada DataFrame carregado, usar o nome base (arquivo.split(".")[0]) como chave
#
# Observa√ß√µes:
#   - Se houver dois arquivos com mesmo nome base mas diferentes extens√µes (ex.: vendas.csv e vendas.xlsx),
#     o dicion√°rio vai sobrescrever uma entrada com a outra. Se isso for um risco, podemos:
#       * Usar chave completa com extens√£o, ou
#       * Agregar em lista (ex.: tabelas.setdefault(nome_base, []).append(df))
#
# ------------------------------------------------------------
# PARTE DE EXIBI√á√ÉO/DEBUG
# ------------------------------------------------------------
# Ap√≥s carregar, o script imprime:
#   - head() de cada DataFrame
#   - df.info()
#   - df.isnull().sum()
#
# Isso serve como inspe√ß√£o inicial (quick check) antes do ETL.
# Em produ√ß√£o, voc√™ normalmente:
#   - gera logs (arquivo de log) em vez de print()
#   - gera m√©tricas (n¬∫ de linhas, colunas, erros) para monitoramento
#
# ------------------------------------------------------------
# BOAS PR√ÅTICAS / PR√ìXIMOS PASSOS SUGERIDOS
# ------------------------------------------------------------
# 1) Logging:
#    - Substituir prints por logging (m√≥dulo logging), com n√≠veis (INFO, WARNING, ERROR).
#    - Salvar em arquivo de logs para auditoria.
#
# 2) Configura√ß√£o:
#    - Receber PASTA via argumento de linha de comando (argparse) ou vari√°veis de ambiente.
#
# 3) Identifica√ß√£o autom√°tica de tabelas (quando nomes variam):
#    - Se arquivos s√£o vari√°veis, implementar heur√≠stica de identifica√ß√£o (ex.: buscar colunas que indicam "vendas",
#      procurar colunas como "valor", "data", "produto", "id_financeiro").
#
# 4) Tratamento de grandes volumes:
#    - Para arquivos que n√£o cabem na mem√≥ria, considerar:
#       * Dask (API compat√≠vel com pandas), ou
#       * PySpark (pyspark.sql), ou
#       * Ler em chunks com pd.read_csv(..., chunksize=...)
#
# 5) Formatos adicionais:
#    - Parquet: pd.read_parquet (muito usado em data engineering por performance)
#    - Feather: pd.read_feather (r√°pido, colunar)
#    - Bancos SQL: pd.read_sql(query, connection)
#
# 6) Valida√ß√µes e testes:
#    - Validar schema esperado (colunas obrigat√≥rias, tipos)
#    - Validar integridade (IDs √∫nicos, chaves estrangeiras)
#    - Implementar testes unit√°rios (pytest) para suas fun√ß√µes ETL
#
# 7) Orquestra√ß√£o:
#    - Em projetos maiores, orquestrar com Airflow, Prefect ou Dagster
#    - dbt √© excelente para transforma√ß√µes SQL em Data Warehouses (ex.: BigQuery/Redshift)
#
# 8) Seguran√ßa:
#    - Cuidado com arquivos vindos de terceiros (inje√ß√£o, formatos maliciosos)
#    - Evitar executar c√≥digo vindo de arquivos (ex.: eval)
#
# 9) Documenta√ß√£o:
#    - Documente o contrato dos arquivos (colunas esperadas, tipos, frequ√™ncia de atualiza√ß√£o)
#
# ------------------------------------------------------------
# DICAS PR√ÅTICAS PARA VOC√ä (estudante / 1¬∫ projeto)
# ------------------------------------------------------------
# - Separar ingest√£o (este script) e transforma√ß√µes (seu ETL) √© √≥tima pr√°tica: modularidade.
# - Sempre rode o script com amostras dos dados primeiro para prevenir surpresas.
# - Use um notebook (Jupyter) para explorar os dados e validar transforma√ß√µes antes de automatizar.
# - Quando pedir ajuda (IA, colegas), sempre traga prints/erros e descreva o que tentou.
#
# ------------------------------------------------------------
# EXEMPLO R√ÅPIDO DE COMO EVOLUIR (ap√≥s este script)
# ------------------------------------------------------------
# - Adicione uma fun√ß√£o `identificar_tabelas(tabelas)` que mapeia cada DataFrame para o papel
#   (ex.: "vendas", "financeiro", "clientes") usando heur√≠sticas de colunas.
# - Crie `etl_padronizacao(df)` que recebe um DF e aplica limpeza padr√£o (strip, lower, trim, converter tipos).
# - Crie `etl_transformacoes(tabelas)` que combina DataFrames e gera o dataset final.
#
# ------------------------------------------------------------
# CONCLUS√ÉO:
# ------------------------------------------------------------
# O c√≥digo que voc√™ criou j√° tem a estrutura b√°sica correta e profissional:
# - modular (fun√ß√µes separadas)
# - tolerante a erros (try/except)
# - pronto para extens√£o (mais formatos, logging, valida√ß√µes)
#
# Cole esses coment√°rios abaixo do seu script para refer√™ncia enquanto trabalhamos no ETL
# e, quando quiser, eu j√° posso come√ßar a escrever as 5 transforma√ß√µes seguindo as melhores pr√°ticas.
#
# Boa! Vamos para o ETL quando voc√™ disser. üöÄ
#
# ------------------------------------------------------------
